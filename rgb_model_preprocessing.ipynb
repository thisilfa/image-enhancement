{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe44f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import face_recognition\n",
    "\n",
    "from cv2 import dnn_superres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc422fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = \"/home/serverai/Project/face-recognition/face_attendance_system/prod/models/result.json\"\n",
    "# MODEL_PATH = \"/home/serverai/Project/face-recognition/face_attendance_system/prod/models/encode_dev.json\"\n",
    "MODEL_PATH = \"/home/serverai/Project/face-recognition/face_attendance_system/prod/models/result-rev2.json\"\n",
    "# MODEL_PATH = \"result.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b66dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MODEL_PATH) as file:\n",
    "    face_encode = json.load(file)\n",
    "list_face_encode = [data[0] for data in face_encode.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e3063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_new = 145\n",
    "res = 141"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d354183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(face_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d02c30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_test = 'images/success/'\n",
    "failed_test = 'images/failed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244266e1",
   "metadata": {},
   "source": [
    "## Necessary Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e6b285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(path):\n",
    "#     list_all = []\n",
    "    for image in os.listdir(path):\n",
    "        if image != \".ipynb_checkpoints\":\n",
    "            print(image)\n",
    "            img_path = f\"{path}/{image}\"\n",
    "            \n",
    "            img = cv2.imread(img_path)\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            face_locations = face_recognition.face_locations(img)\n",
    "            encode_image = face_recognition.face_encodings(img, face_locations)[0]\n",
    "            face_distances = face_recognition.face_distance(list_face_encode, encode_image)\n",
    "            valid_distances = sorted((dist, idx) for idx, dist in enumerate(face_distances) if dist < float(0.35))\n",
    "\n",
    "            if valid_distances:\n",
    "                closest_faces_indices = [idx for _, idx in valid_distances[:5]]\n",
    "                distance = [dist for dist,_ in valid_distances[:5]]\n",
    "\n",
    "                candidates = []\n",
    "                for idx, dist in zip(closest_faces_indices, distance):\n",
    "                    candidate_key = next(key for key, value in face_encode.items() if np.array_equal(value[0], list_face_encode[idx]))\n",
    "                    score = round((1 - dist) * 100, 2)\n",
    "                    candidates.append((candidate_key, score))\n",
    "                print(candidates)\n",
    "            else:\n",
    "                print(\"No valid distances\")\n",
    "            print(\"=====\")\n",
    "#     df = pd.DataFrame(list_all, columns=['name', 'score'])\n",
    "#     return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54254865",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"images/error2\"\n",
    "check_accuracy(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d91ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72399cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65958bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, figsize=(10, 10)):\n",
    "    scores = []\n",
    "    ncol = 5\n",
    "    nrow = len(images) // 5\n",
    "    fig, axes = plt.subplots(nrows=nrow, ncols=ncol, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    for ax, img in zip(axes, images):\n",
    "        candidates = get_face_loc(img)\n",
    "        scores.append(candidates)\n",
    "#         print(f'\\n {candidates}')\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.axis('off')\n",
    "    \n",
    "#     print(\"\\n\")\n",
    "    df = create_df(scores)\n",
    "    print(df)\n",
    "    average = get_avg_value(df)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcc164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face_loc(img_arr):\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    try:\n",
    "        face_locations = face_recognition.face_locations(img_arr)\n",
    "        if not face_locations:\n",
    "            candidates = []\n",
    "        else:\n",
    "            encode_image = face_recognition.face_encodings(img_arr, face_locations, num_jitters=2, model='large')[0]\n",
    "            face_distances = face_recognition.face_distance(list_face_encode, encode_image)\n",
    "            valid_distances = sorted((dist, idx) for idx, dist in enumerate(face_distances) if dist < float(0.35))\n",
    "\n",
    "            if valid_distances:\n",
    "                closest_faces_indices = [idx for _, idx in valid_distances[:5]]\n",
    "                distance = [dist for dist,_ in valid_distances[:5]]\n",
    "\n",
    "                for idx, dist in zip(closest_faces_indices, distance):\n",
    "                    candidate_key = next(key for key, value in face_encode.items() if np.array_equal(value[0], list_face_encode[idx]))\n",
    "                    score = round((1 - dist) * 100, 2)\n",
    "                    candidates.append((candidate_key, score))\n",
    "    except:\n",
    "        print('This image does not contain face(s)')\n",
    "                \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53fa89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(a_list):\n",
    "    df_flat = [item for sublist in a_list for item in sublist]\n",
    "    df = pd.DataFrame(df_flat, columns=['name', 'score'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598cb190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_value(df):\n",
    "    df['name'] = [name.split('-')[0] for name in df.name]\n",
    "    df = df.groupby('name')['score'].mean().reset_index()\n",
    "    df.columns = ['name', 'average_score']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336af70f",
   "metadata": {},
   "source": [
    "## Original vs Flip Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4876fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_rgb_score(img_path):\n",
    "    candidates = []\n",
    "    img = cv2.imread(img_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    face_locations = face_recognition.face_locations(img_rgb)\n",
    "    print(face_locations)\n",
    "    if face_locations:\n",
    "        encode_image = face_recognition.face_encodings(img_rgb, face_locations, num_jitters=2, model='large')[0]\n",
    "        face_distances = face_recognition.face_distance(list_face_encode, encode_image)\n",
    "        valid_distances = sorted((dist, idx) for idx, dist in enumerate(face_distances) if dist < float(0.36))\n",
    "\n",
    "        if valid_distances:\n",
    "            closest_faces_indices = [idx for _, idx in valid_distances[:5]]\n",
    "            distance = [dist for dist,_ in valid_distances[:5]]\n",
    "\n",
    "            for idx, dist in zip(closest_faces_indices, distance):\n",
    "                candidate_key = next(key for key, value in face_encode.items() if np.array_equal(value[0], list_face_encode[idx]))\n",
    "                score = round((1 - dist) * 100, 2)\n",
    "                candidates.append((candidate_key, score))\n",
    "                \n",
    "        print(f\"valid indices \\n {candidates}\")\n",
    "    else:\n",
    "        print(f\"no valid indices\")\n",
    "\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e440625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bgr_score(img_path):\n",
    "    candidates = []\n",
    "    img = cv2.imread(img_path)\n",
    "    face_locations = face_recognition.face_locations(img)\n",
    "    print(face_locations)\n",
    "    if face_locations:\n",
    "        encode_image = face_recognition.face_encodings(img, face_locations, num_jitters=2, model='large')[0]\n",
    "        face_distances = face_recognition.face_distance(list_face_encode, encode_image)\n",
    "        valid_distances = sorted((dist, idx) for idx, dist in enumerate(face_distances) if dist < float(0.36))\n",
    "\n",
    "        if valid_distances:\n",
    "            closest_faces_indices = [idx for _, idx in valid_distances[:5]]\n",
    "            distance = [dist for dist,_ in valid_distances[:5]]\n",
    "\n",
    "            for idx, dist in zip(closest_faces_indices, distance):\n",
    "                candidate_key = next(key for key, value in face_encode.items() if np.array_equal(value[0], list_face_encode[idx]))\n",
    "                score = round((1 - dist) * 100, 2)\n",
    "                candidates.append((candidate_key, score))\n",
    "    \n",
    "        print(f\" valid indices \\n {candidates}\")\n",
    "    else:\n",
    "        print(f\"no valid indices\")\n",
    "\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd44637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scores(folder_path):\n",
    "    bgr_list = []\n",
    "    rgb_list = []\n",
    "    \n",
    "    for img_path in os.listdir(folder_path):\n",
    "        images = os.path.join(folder_path, img_path)\n",
    "        start_time = time.time()\n",
    "        print(f\"\\n image {images} on BGR\")\n",
    "        bgr_scores = check_bgr_score(images)\n",
    "        print(f\"Process time: {round((time.time() - start_time), 2)} \\n ====\")\n",
    "        bgr_list.append(bgr_scores)\n",
    "        start_time = time.time()\n",
    "        print(f\"\\n image {images} on RGB\")\n",
    "        rgb_scores = check_rgb_score(images)\n",
    "        print(f\"Process time: {round((time.time() - start_time), 2)} \\n ====\")\n",
    "        rgb_list.append(rgb_scores)\n",
    "    \n",
    "    return bgr_list, rgb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa64827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bgr_score, rgb_score = load_scores(failed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b30c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rgb = create_df(rgb_score)\n",
    "df_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c22d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_avg_value(df_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e320fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_avg_value(df_rgb)['average_score'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ff548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_bgr = create_df(bgr_score)\n",
    "df_bgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6adec",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_avg_value(df_bgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef891348",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_avg_value(df_bgr)['average_score'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9971812f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b2dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_flip_rgb_score(img_path):\n",
    "    candidates = []\n",
    "    img = cv2.imread(img_path)\n",
    "    flip = cv2.flip(img, 1)\n",
    "    img_rgb = cv2.cvtColor(flip, cv2.COLOR_BGR2RGB)\n",
    "    face_locations = face_recognition.face_locations(img_rgb)\n",
    "    if face_locations:\n",
    "        encode_image = face_recognition.face_encodings(img_rgb, face_locations)[0]\n",
    "        face_distances = face_recognition.face_distance(list_face_encode, encode_image)\n",
    "        valid_distances = sorted((dist, idx) for idx, dist in enumerate(face_distances) if dist < float(0.35))\n",
    "\n",
    "        if valid_distances:\n",
    "            closest_faces_indices = [idx for _, idx in valid_distances[:5]]\n",
    "            distance = [dist for dist,_ in valid_distances[:5]]\n",
    "\n",
    "            for idx, dist in zip(closest_faces_indices, distance):\n",
    "                candidate_key = next(key for key, value in face_encode.items() if np.array_equal(value[0], list_face_encode[idx]))\n",
    "                score = round((1 - dist) * 100, 2)\n",
    "                candidates.append((candidate_key, score))\n",
    "        print(f\"valid indices \\n {candidates}\")\n",
    "    else:\n",
    "        print(f\"no valid indices\")\n",
    "#     print(\"====\")\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab1f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_flip_bgr_score(img_path):\n",
    "    candidates = []\n",
    "    img = cv2.imread(img_path)\n",
    "    flip = cv2.flip(img, 1)\n",
    "    face_locations = face_recognition.face_locations(flip)\n",
    "    if face_locations:\n",
    "        encode_image = face_recognition.face_encodings(flip, face_locations)[0]\n",
    "        face_distances = face_recognition.face_distance(list_face_encode, encode_image)\n",
    "        valid_distances = sorted((dist, idx) for idx, dist in enumerate(face_distances) if dist < float(0.35))\n",
    "\n",
    "        if valid_distances:\n",
    "            closest_faces_indices = [idx for _, idx in valid_distances[:5]]\n",
    "            distance = [dist for dist,_ in valid_distances[:5]]\n",
    "\n",
    "            for idx, dist in zip(closest_faces_indices, distance):\n",
    "                candidate_key = next(key for key, value in face_encode.items() if np.array_equal(value[0], list_face_encode[idx]))\n",
    "                score = round((1 - dist) * 100, 2)\n",
    "                candidates.append((candidate_key, score))\n",
    "        print(f\"valid indices \\n {candidates}\")\n",
    "    else:\n",
    "        print(f\"no valid indices\")\n",
    "#     print(\"====\")\n",
    "\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a2d8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flip_scores(folder_path):\n",
    "    bgr_list = []\n",
    "    rgb_list = []\n",
    "    \n",
    "    for img_path in os.listdir(folder_path):\n",
    "        \n",
    "        images = os.path.join(folder_path, img_path)\n",
    "        start_time = time.time()\n",
    "        print(f\"\\n image {images} on BGR\")\n",
    "        bgr_scores = check_flip_bgr_score(images)\n",
    "        print(f\"Process time: {round((time.time() - start_time), 2)} \\n ====\")\n",
    "        bgr_list.append(bgr_scores)\n",
    "        start_time = time.time()\n",
    "        print(f\"\\n image {images} on RGB\")\n",
    "        rgb_scores = check_flip_rgb_score(images)\n",
    "        print(f\"Process time: {round((time.time() - start_time), 2)} \\n ====\")\n",
    "        rgb_list.append(rgb_scores)\n",
    "    \n",
    "    return bgr_list, rgb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a0bd8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flip_bgr_score, flip_rgb_score = load_flip_scores(failed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef79c04b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_flip_rgb = create_df(flip_rgb_score)\n",
    "df_flip_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018c9db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_avg_value(df_flip_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ec6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_avg_value(df_flip_rgb)['average_score'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3b19a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_flip_bgr = create_df(flip_bgr_score)\n",
    "df_flip_bgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a28ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_avg_value(df_flip_bgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42cb6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_avg_value(df_flip_bgr)['average_score'].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130cc018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e26c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0601d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'images/31#1-revasya/failed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed5b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "\n",
    "for img_path in os.listdir(test):\n",
    "    print(img_path)\n",
    "    path = os.path.join(test, img_path)\n",
    "#     print(path)\n",
    "    rgb_score = check_rgb_score(path)\n",
    "    score.append(rgb_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17955a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0901207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d6ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs = [cv2.imread(os.path.join(failed_test, img)) for img in os.listdir(failed_test)]\n",
    "# img_rgb = [cv2.cvtColor(cv2.imread(os.path.join(failed_test, img)), cv2.COLOR_BGR2RGB) for img in os.listdir(failed_test)]\n",
    "ori_avg = plot_images(ori_imgs)\n",
    "ori_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4ee35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Average Score for Original Images: \\n{ori_avg['average_score'].mean().round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7410a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ori_imgs = [cv2.imread(os.path.join(success_test, img)) for img in os.listdir(success_test)]\n",
    "img_rgb = [cv2.cvtColor(cv2.imread(os.path.join(failed_test, img)), cv2.COLOR_BGR2RGB) for img in os.listdir(failed_test)]\n",
    "ori_avg = plot_images(img_rgb)\n",
    "ori_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fdd004",
   "metadata": {},
   "outputs": [],
   "source": [
    "flip_imgs = []\n",
    "for img_path in os.listdir(failed_test):\n",
    "#     images = cv2.imread(os.path.join(failed_test, img_path))\n",
    "    images = cv2.cvtColor(cv2.imread(os.path.join(failed_test, img_path)), cv2.COLOR_BGR2RGB)\n",
    "    flip = cv2.flip(images, 1)\n",
    "    cv2.cvtColor(flip, cv2.COLOR_BGR2RGB)\n",
    "    flip_imgs.append(flip)\n",
    "flip_avg = plot_images(flip_imgs)\n",
    "flip_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9c8cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Average Score for Flip Original Images: \\n{flip_avg['average_score'].mean().round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4edbc4d",
   "metadata": {},
   "source": [
    "# Normalize image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58b2118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_show_images(images, figsize=(10, 10)):\n",
    "    scores = []\n",
    "    ncol = 5\n",
    "    nrow = len(images) // 5\n",
    "    fig, axes = plt.subplots(nrows=nrow, ncols=ncol, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    for ax, img in zip(axes, images):\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.axis('off')\n",
    "    \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8243cbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = 'images/test/'\n",
    "img_rgb = [cv2.cvtColor(cv2.imread(os.path.join(test, img)), cv2.COLOR_BGR2RGB) for img in os.listdir(test)]\n",
    "for i in img_rgb:\n",
    "    print(get_face_loc(i))\n",
    "only_show_images(img_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fdf5bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fliped = []\n",
    "img_rgb = [cv2.cvtColor(cv2.imread(os.path.join(test, img)), cv2.COLOR_BGR2RGB) for img in os.listdir(test)]\n",
    "for i in img_rgb:\n",
    "    i = cv2.flip(i, 1)\n",
    "    fliped.append(i)\n",
    "    print(get_face_loc(i))\n",
    "only_show_images(fliped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb416ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampling = []\n",
    "for i in sr_imgs:\n",
    "    rows, cols, _channels = map(int, i.shape)\n",
    "    ups = cv2.pyrUp(i, dstsize=(2 * cols, 2 * rows))\n",
    "    cv2.cvtColor(ups, cv2.COLOR_BGR2RGB)\n",
    "    upsampling.append(ups)\n",
    "    print(get_face_loc(ups))\n",
    "only_show_images(upsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9dd1f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "norm_imgs = []\n",
    "for i in sr_imgs:\n",
    "# #     images = cv2.imread(os.path.join(failed_test, img_path))\n",
    "#     images = cv2.cvtColor(cv2.imread(os.path.join(test, img_path)), cv2.COLOR_BGR2RGB)\n",
    "    normi = cv2.normalize(i, None, alpha=0, beta=355, norm_type=cv2.NORM_MINMAX)\n",
    "    cv2.cvtColor(normi, cv2.COLOR_BGR2RGB)\n",
    "    norm_imgs.append(normi)\n",
    "only_show_images(norm_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4dedd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in norm_imgs:\n",
    "    print(get_face_loc(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a38242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c21844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c05a787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0677b495",
   "metadata": {},
   "source": [
    "## Sharpness Enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef4645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_sharpness(image, kernel):\n",
    "    sharp_image = cv2.filter2D(image, -1, kernel)\n",
    "    return sharp_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102fd764",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = {\n",
    "    'mild': np.array([\n",
    "        [0, -0.5, 0],\n",
    "        [-0.5, 3, -0.5],\n",
    "        [0, -0.5, 0]\n",
    "    ]),\n",
    "    'normal': np.array([\n",
    "        [0, -1, 0],\n",
    "        [-1, 5, -1],\n",
    "        [0, -1, 0]\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd90812",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sharp_normal_img = []\n",
    "for i in norm_imgs:\n",
    "    start_time = time.time()\n",
    "    sharpen = enhance_sharpness(i, kernels['mild'])\n",
    "    print(f\"Process image in time: {round((time.time() - start_time), 2)} \\n ====\")\n",
    "    sharp_normal_img.append(sharpen)\n",
    "sn_avg = plot_images(sharp_normal_img)\n",
    "sn_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2be6d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Average Score for Normal Sharp Flip Images: \\n{sn_avg['average_score'].mean().round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa159d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sharp_mild_img = []\n",
    "for i in flip_imgs:\n",
    "    start_time = time.time()\n",
    "    sharpen = enhance_sharpness(i, kernels['mild'])\n",
    "    print(f\"Process image in time: {round((time.time() - start_time), 2)} \\n ====\")\n",
    "    sharp_mild_img.append(sharpen)\n",
    "sm_avg = plot_images(sharp_mild_img)\n",
    "sm_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1f5af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Average Score for Mild Sharp Flip Images: \\n{sm_avg['average_score'].mean().round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d4ce8",
   "metadata": {},
   "source": [
    "## Auto Brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8adfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = {\n",
    "    'low_contrast': 0.5,\n",
    "    'medium_contrast': 0.75,\n",
    "    'high_contrast': 1.0,\n",
    "    'xtra_contrast': 2.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bbff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_brightness(img, limit):\n",
    "    \n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=limit, tileGridSize=(4, 4))\n",
    "    cl = clahe.apply(l)\n",
    "\n",
    "    limg = cv2.merge((cl, a, b))\n",
    "    result = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142799e1",
   "metadata": {},
   "source": [
    " ### from Original Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068dde48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "flip_brightness = []\n",
    "\n",
    "for i in norm_imgs:\n",
    "    start_time = time.time()\n",
    "    lab = auto_brightness(i, limit['medium_contrast'])\n",
    "    print(f\"Process image in time: {round((time.time() - start_time), 2)} \\n ====\")\n",
    "    flip_brightness.append(lab)\n",
    "auto_flip_avg = plot_images(flip_brightness)\n",
    "auto_flip_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b749daaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Average Score for Brighter Original Images: \\n{auto_flip_avg['average_score'].mean().round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eb954d",
   "metadata": {},
   "source": [
    " ### from Sharp Normal Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7cd979",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_brightness = []\n",
    "\n",
    "for i in sharp_normal_img:\n",
    "    start_time = time.time()\n",
    "    lab = auto_brightness(i, limit['medium_contrast'])\n",
    "    print(f\"Process image in time: {round((time.time() - start_time), 2)} \\n ====\")\n",
    "    sn_brightness.append(lab)\n",
    "auto_sn_avg = plot_images(sn_brightness)\n",
    "auto_sn_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9586bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Average Score for Brighter Sharp Normal Images: \\n{auto_sn_avg['average_score'].mean().round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e832ad4",
   "metadata": {},
   "source": [
    " ### from Sharp Mild Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269640c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_brightness = []\n",
    "\n",
    "for i in sharp_mild_img:\n",
    "    start_time = time.time()\n",
    "    lab = auto_brightness(i, limit['medium_contrast'])\n",
    "    print(f\"Process image {i} in time: {round((time.time() - start_time), 2)} \\n ====\")\n",
    "    sm_brightness.append(lab)\n",
    "auto_sm_avg = plot_images(sm_brightness)\n",
    "auto_sm_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21cde84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Average Score for Brighter Sharp Normal Images: \\n{auto_sm_avg['average_score'].mean().round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9813e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b94b39a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b372fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('path_to_your_image.jpg')\n",
    "\n",
    "def upscale(img, scale=2)\n",
    "scale_factor = 2\n",
    "\n",
    "# Get the original dimensions\n",
    "height, width = image.shape[:2]\n",
    "\n",
    "# Compute new dimensions\n",
    "new_width = int(width * scale_factor)\n",
    "new_height = int(height * scale_factor)\n",
    "\n",
    "# Resize the image\n",
    "resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d9031e",
   "metadata": {},
   "source": [
    "# Another Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154a335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"images/1722210090_.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c991c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(img_path)\n",
    "img = cv2.flip(img, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4534271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_lab = auto_brightness(img, limit['medium_contrast'])\n",
    "img_rgb = cv2.cvtColor(img_lab, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a13add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_locations = face_recognition.face_locations(img_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485bd4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_image = face_recognition.face_encodings(img_lab, face_locations)[0]\n",
    "encode_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb4ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_distances = face_recognition.face_distance(list_face_encode, encode_image)\n",
    "face_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e04272",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_distances = sorted((dist, idx) for idx, dist in enumerate(face_distances) if dist < float(0.36))\n",
    "valid_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3e034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, dist in enumerate(face_distances):\n",
    "    if dist < float(0.45):\n",
    "        print(sorted((dist, idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aad57c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd10c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_rgb_score('images/1722317218_JORGI VEGASERRA PARFIN.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62671445",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_bgr_score('images/1722317218_JORGI VEGASERRA PARFIN.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694fc1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_flip_rgb_score('images/1722317218_JORGI VEGASERRA PARFIN.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed9c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_flip_bgr_score('images/1722317218_JORGI VEGASERRA PARFIN.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb16fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = auto_brightness(img_rgb, limit['high_contrast'])\n",
    "fig, axes = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "axes.imshow(i, cmap='gray')\n",
    "axes.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6ec65c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c51b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85170ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275c413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spesific_encode(name, dic):\n",
    "    result = []\n",
    "\n",
    "    def search_dict(d, name=name):\n",
    "        for key, value in d.items():\n",
    "            all_names = re.findall(r'\\w+', key)[1]\n",
    "            if name in all_names:\n",
    "                print\n",
    "                result.append({key: value})\n",
    "            if isinstance(value, dict):\n",
    "                search_dict(value)\n",
    "\n",
    "    search_dict(dic)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459fb07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "revasya = get_spesific_encode('REVASYA', face_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b404a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "revasya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe7377b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a94f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0145e4af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9135df5",
   "metadata": {},
   "source": [
    "# Advanced Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae4863b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a456932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e66fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_src = cv2.imread('images/good-img/NUGROHO.jpg')\n",
    "img_src_rgb = cv2.cvtColor(img_src, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f6d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols, _channels = map(int, img_src_rgb.shape)\n",
    "img_src_rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db3bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_src_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ff49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_face_loc(img_src_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9dd4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fcd57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ups = cv2.cvtColor(cv2.pyrUp(img_src, dstsize=(2 * cols, 2 * rows)), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "ups.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf7f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ups)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a2ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_face_loc(ups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfce4e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols, _channels = map(int, ups.shape)\n",
    "ups.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb52ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ups2 = cv2.cvtColor(cv2.pyrUp(ups, dstsize=(2 * cols, 2 * rows)), cv2.COLOR_BGR2RGB)\n",
    "ups2 = cv2.cvtColor(ups2, cv2.COLOR_BGR2RGB)\n",
    "ups2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0af9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ups2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda3ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_face_loc(ups2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bde2a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian = cv2.GaussianBlur(norm_img, (5, 5), 0)\n",
    "plt.imshow(gaussian)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc5ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_img = cv2.normalize(lab, None, alpha=0, beta=355, norm_type=cv2.NORM_MINMAX)\n",
    "print(norm_img.shape)\n",
    "\n",
    "plt.imshow(norm_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf1ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_face_loc(norm_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9633526e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e283f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abddfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = auto_brightness(result, limit['medium_contrast'])\n",
    "print(lab.shape)\n",
    "\n",
    "plt.imshow(lab)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48b9eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_face_loc(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd51f489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da52ae23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51842fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3280c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpen = enhance_sharpness(lab, kernels['mild'])\n",
    "cv2.imwrite(\"images/sharpness.jpg\", sharpen)\n",
    "plt.imshow(sharpen)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8859e290",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_face_loc(sharpen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc4923",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b4e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = 'images/test/1721434068_SHAFIRA NINDY HAPSARI.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885cce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.cvtColor(cv2.imread(image), cv2.COLOR_BGR2RGB)\n",
    "print(img.shape)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395ef48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "superres = dnn_superres.DnnSuperResImpl_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb46cbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ['weights/FSRCNN-small_x2.pb', 'weights/FSRCNN-small_x3.pb', 'weights/FSRCNN-small_x4.pb', \n",
    "        'weights/FSRCNN_x2.pb', 'weights/FSRCNN_x3.pb', 'weights/FSRCNN_x4.pb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0960f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "superres.readModel(path[3])\n",
    "\n",
    "superres.setModel(\"fsrcnn\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a0a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upscale the image\n",
    "start_time = time.time()\n",
    "result = superres.upsample(img)\n",
    "\n",
    "print(f\"Process image in time: {round((time.time() - start_time), 2)} \\n ====\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debdc27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.shape)\n",
    "plt.imshow(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c28673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef7c987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5382f7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c168475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb40354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def superres(image, scale):\n",
    "    path = 'weights/FSRCNN_x2.pb'\n",
    "    \n",
    "    superres = dnn_superres.DnnSuperResImpl_create()\n",
    "    superres.readModel(path)\n",
    "    superres.setModel(\"fsrcnn\", scale)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = superres.upsample(image)\n",
    "\n",
    "    print(f\"Process image in time: {round((time.time() - start_time), 2)} \\n ====\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc00a790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d485b030",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'images/test/'\n",
    "\n",
    "ori_imgs = []\n",
    "for img_path in os.listdir(test):\n",
    "#     images = cv2.imread(os.path.join(failed_test, img_path))\n",
    "    images = cv2.cvtColor(cv2.imread(os.path.join(test, img_path)), cv2.COLOR_BGR2RGB)\n",
    "    ori_imgs.append(images)\n",
    "only_show_images(ori_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f9d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ori_imgs:\n",
    "    print(get_face_loc(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f431125c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = 'images/test/'\n",
    "\n",
    "sr_imgs = []\n",
    "for i in ori_imgs:\n",
    "    sr_img = superres(i, 2)\n",
    "    sr_imgs.append(sr_img)\n",
    "only_show_images(sr_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3badb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sr_imgs:\n",
    "    print(get_face_loc(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c47d00c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ef0704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54d7a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74ffb475",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8a31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import imageio\n",
    "\n",
    "def save_to(img_arr, src_path):\n",
    "    filename = f\"{src_path.split('.')[0]}_changed.JPG\"\n",
    "    image_file = io.BytesIO()\n",
    "    imageio.imwrite(image_file, img_arr, format='jpg')\n",
    "    image_file.seek(0)\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(image_file.getbuffer())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d73766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(img_path):\n",
    "    start_time = time.time()\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    path = 'weights/FSRCNN_x3.pb'\n",
    "    \n",
    "    superres = dnn_superres.DnnSuperResImpl_create()\n",
    "    superres.readModel(path)\n",
    "    superres.setModel(\"fsrcnn\", 3)\n",
    "    \n",
    "    img_rgb = superres.upsample(img_rgb)\n",
    "    \n",
    "    img_rgb = cv2.normalize(img_rgb, None, alpha=0, beta=320, norm_type=cv2.NORM_MINMAX)\n",
    "    cv2.cvtColor(img_rgb, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "#     img_rgb = enhance_sharpness(img_rgb, kernels['mild'])\n",
    "#     img_rgb = cv2.GaussianBlur(img_rgb, (3, 3), 0)\n",
    "    \n",
    "    print(f\"Process {img_path} in time: {round((time.time() - start_time), 2)} \\n ====\")\n",
    "    save_to(img_rgb, img_path)\n",
    "    \n",
    "    return img_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da749f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_test = \"images/error2\"\n",
    "\n",
    "final = []\n",
    "for imgs in os.listdir(dir_test):\n",
    "    img_path = os.path.join(dir_test, imgs)\n",
    "    preprocessed = preprocessing(img_path)\n",
    "    final.append(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2509ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in final:\n",
    "    print(get_face_loc(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ea3559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downscale_image(image, scale):\n",
    "    width, height = image.size\n",
    "    return image.resize((width // scale, height // scale), Image.BICUBIC)\n",
    "\n",
    "def prepare_data(data_dir, output_dir, scale=4):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            img = Image.open(os.path.join(data_dir, filename))\n",
    "            lr_img = downscale_image(img, scale)\n",
    "            lr_img.save(os.path.join(output_dir, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce6e7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirs = 'images/failed'\n",
    "lowr_dirs = 'images/downsample'\n",
    "\n",
    "prepare_data(data_dirs, lowr_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764d82ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs = [cv2.imread(os.path.join(lowr_dirs, img)) for img in os.listdir(lowr_dirs)]\n",
    "# img_rgb = [cv2.cvtColor(cv2.imread(os.path.join(latest, img)), cv2.COLOR_BGR2RGB) for img in os.listdir(latest)]\n",
    "ori_avg = plot_images(ori_imgs)\n",
    "ori_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d10689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "def build_generator():\n",
    "    def residual_block(input):\n",
    "        x = Conv2D(64, (3, 3), padding='same')(input)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "        x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        return tf.keras.layers.add([input, x])\n",
    "    \n",
    "    input = Input(shape=(None, None, 3))\n",
    "    x = Conv2D(64, (9, 9), padding='same')(input)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    for _ in range(16):\n",
    "        x = residual_block(x)\n",
    "    \n",
    "    x = UpSampling2D(size=2)(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv2D(3, (9, 9), padding='same')(x)\n",
    "    output = tf.keras.layers.Activation('tanh')(x)\n",
    "    \n",
    "    return Model(input, output)\n",
    "\n",
    "generator = build_generator()\n",
    "generator.compile(optimizer=Adam(0.0002, 0.5), loss='mse')\n",
    "generator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f39fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dlib_recognition]",
   "language": "python",
   "name": "conda-env-dlib_recognition-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
